{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPT3toXOf4g2NV9vTfelPb3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cralsic123/Siamese-network-object-detection/blob/main/Local_Matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of folder names to copy\n",
        "folder_names = [\"Anchor\", \"Positive\", \"Negative\"]\n",
        "\n",
        "# Destination path in Colab\n",
        "destination_path = \"/content\"\n",
        "\n",
        "# Loop through each folder and copy it to Colab\n",
        "for folder_name in folder_names:\n",
        "    # Path to the folder in Google Drive\n",
        "    folder_path = f\"/content/drive/MyDrive/Places/{folder_name}\"\n",
        "\n",
        "    # Copy the folder from Google Drive to Colab\n",
        "    shutil.copytree(folder_path, os.path.join(destination_path, folder_name))\n",
        "\n",
        "# List the contents of the destination path to verify the folders have been copied\n",
        "print(\"Contents of destination path:\")\n",
        "print(os.listdir(destination_path))\n"
      ],
      "metadata": {
        "id": "IhPqQkr5Pl84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7427a1a2-dda1-452b-c5f7-72fc3fa0fe79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of destination path:\n",
            "['.config', 'Negative', 'Anchor', 'Positive', 'drive', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "from keras import layers\n",
        "from keras import losses\n",
        "from keras import optimizers\n",
        "from keras import metrics\n",
        "from keras import Model\n",
        "from keras.applications import ResNet50\n",
        "\n",
        "target_shape = (200, 200)\n",
        "\n",
        "def preprocess_image(filename):\n",
        "    \"\"\"\n",
        "    Load the specified file as a JPEG image, preprocess it, and\n",
        "    resize it to the target shape.\n",
        "    \"\"\"\n",
        "    image_string = tf.io.read_file(filename)\n",
        "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, target_shape)\n",
        "    return image\n",
        "\n",
        "def preprocess_triplets(anchor, positive, negative):\n",
        "    \"\"\"\n",
        "    Given the filenames corresponding to the three images, load and\n",
        "    preprocess them.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        preprocess_image(anchor),\n",
        "        preprocess_image(positive),\n",
        "        preprocess_image(negative),\n",
        "    )\n",
        "\n",
        "# Load image paths from directories\n",
        "anchor_images_path = \"/content/Anchor\"\n",
        "positive_images_path = \"/content/Positive\"\n",
        "negative_images_path = \"/content/Negative\"\n",
        "\n",
        "anchor_images = sorted([os.path.join(anchor_images_path, f) for f in os.listdir(anchor_images_path)])\n",
        "positive_images = sorted([os.path.join(positive_images_path, f) for f in os.listdir(positive_images_path)])\n",
        "negative_images = sorted([os.path.join(negative_images_path, f) for f in os.listdir(negative_images_path)])\n",
        "\n",
        "image_count = len(anchor_images)\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "anchor_dataset = tf.data.Dataset.from_tensor_slices(anchor_images)\n",
        "positive_dataset = tf.data.Dataset.from_tensor_slices(positive_images)\n",
        "negative_dataset = tf.data.Dataset.from_tensor_slices(negative_images)\n",
        "\n",
        "# Shuffle and preprocess the datasets\n",
        "anchor_dataset = anchor_dataset.shuffle(buffer_size=len(anchor_images))\n",
        "positive_dataset = positive_dataset.shuffle(buffer_size=len(positive_images))\n",
        "negative_dataset = negative_dataset.shuffle(buffer_size=len(negative_images))\n",
        "\n",
        "dataset = tf.data.Dataset.zip((anchor_dataset, positive_dataset, negative_dataset))\n",
        "dataset = dataset.shuffle(buffer_size=1024)\n",
        "dataset = dataset.map(preprocess_triplets)\n",
        "\n",
        "# Split dataset into train and validation sets\n",
        "train_dataset = dataset.take(round(image_count * 0.8))\n",
        "val_dataset = dataset.skip(round(image_count * 0.8))\n",
        "\n",
        "train_dataset = train_dataset.batch(32, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(32, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "def visualize(anchor, positive, negative):\n",
        "    \"\"\"Visualize a few triplets from the supplied batches.\"\"\"\n",
        "    def show(ax, image):\n",
        "        ax.imshow(image)\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    fig = plt.figure(figsize=(9, 9))\n",
        "    axs = fig.subplots(3, 3)\n",
        "    for i in range(3):\n",
        "        show(axs[i, 0], anchor[i])\n",
        "        show(axs[i, 1], positive[i])\n",
        "        show(axs[i, 2], negative[i])\n",
        "\n",
        "visualize(*list(train_dataset.take(1).as_numpy_iterator())[0])\n",
        "\n",
        "\"\"\"def visualize_attention(anchor, positive, negative):\n",
        "    def show(ax, image):\n",
        "        ax.imshow(image)\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 12))\n",
        "    axs = fig.subplots(3, 6)\n",
        "    for i in range(3):\n",
        "        show(axs[i, 0], anchor[i])\n",
        "        show(axs[i, 1], positive[i])\n",
        "        show(axs[i, 2], negative[i])\n",
        "        # Show attention maps\n",
        "        for j, img in enumerate([anchor[i], positive[i], negative[i]]):\n",
        "            ax = axs[i, j+3]\n",
        "            ax.imshow(img)\n",
        "            ax.imshow(attention_map[i, j], cmap='jet', alpha=0.4)\n",
        "            ax.get_xaxis().set_visible(False)\n",
        "            ax.get_yaxis().set_visible(False)\"\"\"\n",
        "\n",
        "#visualize(*list(train_dataset.take(1).as_numpy_iterator())[0])\n",
        "\n",
        "# Define the Siamese network with attention\n",
        "base_cnn = ResNet50(weights=\"imagenet\", input_shape=target_shape + (3,), include_top=False)\n",
        "\n",
        "flatten = layers.Flatten()(base_cnn.output)\n",
        "dense1 = layers.Dense(512, activation=\"relu\")(flatten)\n",
        "dense1 = layers.BatchNormalization()(dense1)\n",
        "dense2 = layers.Dense(256, activation=\"relu\")(dense1)\n",
        "dense2 = layers.BatchNormalization()(dense2)\n",
        "output = layers.Dense(256)(dense2)\n",
        "\n",
        "embedding = Model(base_cnn.input, output, name=\"Embedding\")\n",
        "\n",
        "trainable = False\n",
        "for layer in base_cnn.layers:\n",
        "    if layer.name == \"conv5_block1_out\":\n",
        "        trainable = True\n",
        "    layer.trainable = trainable\n",
        "\n",
        "class DistanceLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    This layer is responsible for computing the distance between the anchor\n",
        "    embedding and the positive embedding, and the anchor embedding and the\n",
        "    negative embedding.\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, anchor, positive, negative):\n",
        "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n",
        "        an_distance = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n",
        "        return (ap_distance, an_distance)\n",
        "\n",
        "anchor_input = layers.Input(shape=target_shape + (3,), name=\"anchor\")\n",
        "positive_input = layers.Input(shape=target_shape + (3,), name=\"positive\")\n",
        "negative_input = layers.Input(shape=target_shape + (3,), name=\"negative\")\n",
        "\n",
        "anchor_embedding = embedding(anchor_input)\n",
        "positive_embedding = embedding(positive_input)\n",
        "negative_embedding = embedding(negative_input)\n",
        "\n",
        "distances = DistanceLayer()(\n",
        "    anchor_embedding,\n",
        "    positive_embedding,\n",
        "    negative_embedding\n",
        ")\n",
        "\n",
        "siamese_network = Model(\n",
        "    inputs=[anchor_input, positive_input, negative_input],\n",
        "    outputs=distances\n",
        ")\n",
        "\n",
        "# Instantiate the Siamese model with attention\n",
        "siamese_model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=distances)\n",
        "\n",
        "class SiameseModelWithAttention(Model):\n",
        "    \"\"\"Siamese Network model with attention.\"\"\"\n",
        "\n",
        "    def __init__(self, siamese_network_with_attention, margin=0.5):\n",
        "        super().__init__()\n",
        "        self.siamese_network_with_attention = siamese_network_with_attention\n",
        "        self.margin = margin\n",
        "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.siamese_network_with_attention(inputs)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self._compute_loss(data)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.siamese_network_with_attention.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.siamese_network_with_attention.trainable_weights))\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        loss = self._compute_loss(data)\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def _compute_loss(self, data):\n",
        "        ap_distance, an_distance = self.siamese_network_with_attention(data)\n",
        "        loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)\n",
        "        return loss\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker]\n",
        "\n",
        "siamese_model_with_attention = SiameseModelWithAttention(siamese_network)\n",
        "siamese_model_with_attention.compile(optimizer=optimizers.Adam(0.0001))\n",
        "\n",
        "# Train the Siamese model with attention\n",
        "siamese_model_with_attention.fit(train_dataset, epochs=4, validation_data=val_dataset)\n",
        "\n",
        "# Visualize the attention maps\n",
        "sample = next(iter(train_dataset))\n",
        "#visualize_attention(*sample)\n"
      ],
      "metadata": {
        "id": "hb7BfhQrPq4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sk7IY_KPTNK"
      },
      "outputs": [],
      "source": [
        "# Step 3: Introduce an attention mechanism\n",
        "# Define the attention mechanism\n",
        "def compute_attention_map(model, img_array):\n",
        "    \"\"\"\n",
        "    Compute the attention map using Grad-CAM or LRP.\n",
        "    \"\"\"\n",
        "    # Perform preprocessing on the image array if necessary\n",
        "    # Example: img_array = preprocess_input(img_array)\n",
        "\n",
        "    # Obtain the last convolutional layer of the model\n",
        "    last_conv_layer = model.get_layer(\"last_conv_layer\")\n",
        "\n",
        "    # Compute gradients of the target class with respect to the output feature map\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_output = model(img_array)\n",
        "        if isinstance(conv_output, tuple):\n",
        "            conv_output = conv_output[0]\n",
        "        target_class = tf.argmax(conv_output, axis=-1)\n",
        "        grads = tape.gradient(conv_output, last_conv_layer.output)\n",
        "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # Compute the weighted sum of the feature maps and normalize\n",
        "    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, last_conv_layer.output), axis=-1)\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.reduce_max(heatmap)\n",
        "\n",
        "    return heatmap\n",
        "\n",
        "# Load the pre-trained model\n",
        "from keras.applications import ResNet50\n",
        "\n",
        "pretrained_model = ResNet50(weights='imagenet', include_top=False, input_shape=target_shape + (3,))\n",
        "# Load your pre-trained model here\n",
        "\n",
        "# Define the Siamese network with attention mechanism\n",
        "base_cnn = resnet.ResNet50(\n",
        "    weights=\"imagenet\", input_shape=target_shape + (3,), include_top=False\n",
        ")\n",
        "\n",
        "flatten = layers.Flatten()(base_cnn.output)\n",
        "dense1 = layers.Dense(512, activation=\"relu\")(flatten)\n",
        "dense1 = layers.BatchNormalization()(dense1)\n",
        "dense2 = layers.Dense(256, activation=\"relu\")(dense1)\n",
        "dense2 = layers.BatchNormalization()(dense2)\n",
        "output = layers.Dense(256)(dense2)\n",
        "\n",
        "# Output layer for attention mechanism\n",
        "output_attention = layers.Conv2D(1, (1, 1), name=\"attention_output\")(base_cnn.output)\n",
        "\n",
        "# Define the Siamese network model\n",
        "siamese_network = Model(\n",
        "    inputs=base_cnn.input,\n",
        "    outputs=[output, output_attention],  # Output feature vector and attention map\n",
        "    name=\"SiameseWithAttention\"\n",
        ")\n",
        "\n",
        "# Define the Siamese model\n",
        "class SiameseModelWithAttention(Model):\n",
        "    def __init__(self, siamese_network, margin=0.5):\n",
        "        super().__init__()\n",
        "        self.siamese_network = siamese_network\n",
        "        self.margin = margin\n",
        "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.siamese_network(inputs)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Your training step implementation\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self._compute_loss(data)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
        "        self.optimizer.apply_gradients(\n",
        "            zip(gradients, self.siamese_network.trainable_weights)\n",
        "        )\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "        pass\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Your testing step implementation\n",
        "        loss = self._compute_loss(data)\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "        pass\n",
        "\n",
        "    def _compute_loss(self, data):\n",
        "        # Your loss computation implementation\n",
        "        ap_distance, an_distance = self.siamese_network(data)\n",
        "        loss = ap_distance - an_distance + self.margin\n",
        "        loss = tf.maximum(loss, 0.0)\n",
        "        return loss\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker]\n",
        "\n",
        "# Instantiate the Siamese model with attention\n",
        "siamese_model_with_attention = SiameseModelWithAttention(siamese_network)\n",
        "# Compile the Siamese model with attention\n",
        "siamese_model_with_attention.compile(optimizer=optimizers.Adam(0.0001))\n",
        "\n",
        "# Train the Siamese model with attention\n",
        "siamese_model_with_attention.fit(train_dataset, epochs=3, validation_data=val_dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "bVu2hVChP50F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSqaoRFyP86J",
        "outputId": "0def7bbe-d466-461b-9377-ecbbd377b3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.1.27-py3-none-any.whl (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.2/721.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.17.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Collecting thop>=0.1.1 (from ultralytics)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, thop, ultralytics\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 thop-0.1.1.post2209072238 ultralytics-8.1.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "0oRKO555QBlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ultralytics\n",
        "ultralytics.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WUI-M3GdRgxV",
        "outputId": "99e46fe7-1674-45ab-c582-0e796912df90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'8.1.27'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics.vit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVeq9mDnTjhh",
        "outputId": "45c53416-3cae-440e-d87d-d9c713dc6832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement ultralytics.vit (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for ultralytics.vit\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import SAM"
      ],
      "metadata": {
        "id": "YaBSDoBzRmXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SAM('sam_b.pt')\n",
        "model.predict('/content/Negative/gsun_01c2a2bdc3dd21273daaa508828df0c5.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBp3hGnERorn",
        "outputId": "f111c96a-2947-406c-e2b5-a6f9fcf72d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/Negative/gsun_01c2a2bdc3dd21273daaa508828df0c5.jpg: 1024x1024 1 0, 1 1, 1 2, 1 3, 1 4, 1 5, 1 6, 1 7, 1 8, 1 9, 1 10, 1 11, 1 12, 1 13, 1 14, 1 15, 1 16, 1 17, 1 18, 1 19, 1 20, 1 21, 1 22, 1 23, 1 24, 1 25, 1 26, 1 27, 1 28, 1 29, 1 30, 1 31, 1 32, 1 33, 1 34, 1 35, 1 36, 1 37, 1 38, 1 39, 1 40, 1 41, 1 42, 1 43, 1 44, 1 45, 1 46, 1 47, 1 48, 1 49, 1 50, 1 51, 1 52, 1 53, 1 54, 1 55, 1 56, 1 57, 1 58, 1 59, 1 60, 1 61, 1 62, 1 63, 1 64, 1 65, 1 66, 1 67, 1 68, 1 69, 1 70, 1 71, 10279.7ms\n",
            "Speed: 6.2ms preprocess, 10279.7ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ultralytics.engine.results.Results object with attributes:\n",
              " \n",
              " boxes: ultralytics.engine.results.Boxes object\n",
              " keypoints: None\n",
              " masks: ultralytics.engine.results.Masks object\n",
              " names: {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9', 10: '10', 11: '11', 12: '12', 13: '13', 14: '14', 15: '15', 16: '16', 17: '17', 18: '18', 19: '19', 20: '20', 21: '21', 22: '22', 23: '23', 24: '24', 25: '25', 26: '26', 27: '27', 28: '28', 29: '29', 30: '30', 31: '31', 32: '32', 33: '33', 34: '34', 35: '35', 36: '36', 37: '37', 38: '38', 39: '39', 40: '40', 41: '41', 42: '42', 43: '43', 44: '44', 45: '45', 46: '46', 47: '47', 48: '48', 49: '49', 50: '50', 51: '51', 52: '52', 53: '53', 54: '54', 55: '55', 56: '56', 57: '57', 58: '58', 59: '59', 60: '60', 61: '61', 62: '62', 63: '63', 64: '64', 65: '65', 66: '66', 67: '67', 68: '68', 69: '69', 70: '70', 71: '71'}\n",
              " obb: None\n",
              " orig_img: array([[[3, 5, 6],\n",
              "         [3, 5, 6],\n",
              "         [2, 4, 5],\n",
              "         ...,\n",
              "         [0, 2, 3],\n",
              "         [0, 2, 3],\n",
              "         [0, 1, 2]],\n",
              " \n",
              "        [[3, 5, 6],\n",
              "         [3, 5, 6],\n",
              "         [3, 5, 6],\n",
              "         ...,\n",
              "         [0, 2, 3],\n",
              "         [0, 2, 3],\n",
              "         [0, 2, 3]],\n",
              " \n",
              "        [[3, 5, 6],\n",
              "         [3, 5, 6],\n",
              "         [3, 5, 6],\n",
              "         ...,\n",
              "         [0, 2, 3],\n",
              "         [0, 2, 3],\n",
              "         [0, 2, 3]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0, 1, 2],\n",
              "         [0, 1, 2],\n",
              "         [0, 1, 5],\n",
              "         ...,\n",
              "         [0, 0, 5],\n",
              "         [0, 0, 4],\n",
              "         [0, 0, 4]],\n",
              " \n",
              "        [[0, 0, 1],\n",
              "         [1, 0, 2],\n",
              "         [0, 1, 5],\n",
              "         ...,\n",
              "         [1, 2, 6],\n",
              "         [0, 0, 4],\n",
              "         [0, 0, 4]],\n",
              " \n",
              "        [[1, 0, 2],\n",
              "         [2, 1, 3],\n",
              "         [0, 1, 5],\n",
              "         ...,\n",
              "         [0, 1, 5],\n",
              "         [0, 0, 4],\n",
              "         [0, 0, 4]]], dtype=uint8)\n",
              " orig_shape: (256, 256)\n",
              " path: '/content/Negative/gsun_01c2a2bdc3dd21273daaa508828df0c5.jpg'\n",
              " probs: None\n",
              " save_dir: 'runs/segment/predict'\n",
              " speed: {'preprocess': 6.217718124389648, 'inference': 10279.69765663147, 'postprocess': 3.032684326171875}]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Define the directory containing your images\n",
        "image_dir = \"/content/Positive\"\n",
        "\n",
        "# Create a directory to save images with bounding boxes\n",
        "output_dir = \"/content/output\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Loop through each image in the directory\n",
        "for filename in os.listdir(image_dir):\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Assuming images are in JPG or PNG format\n",
        "        image_path = os.path.join(image_dir, filename)\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Convert the image to the HSV color space\n",
        "        hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "        # Define lower and upper bounds for the color you want to segment (e.g., green)\n",
        "        lower_bound = np.array([40, 40, 40])  # Adjust these values based on the color you want to detect\n",
        "        upper_bound = np.array([70, 255, 255])\n",
        "\n",
        "        # Create a mask using the specified color range\n",
        "        mask = cv2.inRange(hsv_image, lower_bound, upper_bound)\n",
        "\n",
        "        # Find contours in the mask\n",
        "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        # Create a bounding box around each contour and draw it on the original image\n",
        "        for contour in contours:\n",
        "            x, y, w, h = cv2.boundingRect(contour)\n",
        "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "        # Save the image with bounding boxes\n",
        "        output_path = os.path.join(output_dir, filename)\n",
        "        cv2.imwrite(output_path, image)\n"
      ],
      "metadata": {
        "id": "mo75apySToLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y9DU4JXZVJeX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}